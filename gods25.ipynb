{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10760483,"sourceType":"datasetVersion","datasetId":6673545}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Cell 1: Install required packages\n!pip install transformers\n!pip install accelerate -U  # Required for proper model training\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T13:58:18.949162Z","iopub.execute_input":"2025-02-16T13:58:18.949513Z","iopub.status.idle":"2025-02-16T13:58:25.705999Z","shell.execute_reply.started":"2025-02-16T13:58:18.949483Z","shell.execute_reply":"2025-02-16T13:58:25.704720Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.28.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.3.0)\nRequirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.2)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.5.1+cu121)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.28.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.5)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.17.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2024.9.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<3.0.0,>=1.17->accelerate) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.1.31)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import pandas as pd\nimport torch\nimport numpy as np\nimport re\nfrom pathlib import Path\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import (\n    RobertaTokenizer,\n    RobertaForSequenceClassification,\n    AdamW,\n    get_linear_schedule_with_warmup\n)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, accuracy_score\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom tqdm import tqdm\nimport pandas as pd\nimport numpy as np\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport nltk\nfrom string import punctuation\nimport matplotlib.pyplot as plt\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T13:58:25.707503Z","iopub.execute_input":"2025-02-16T13:58:25.707768Z","iopub.status.idle":"2025-02-16T13:58:25.713665Z","shell.execute_reply.started":"2025-02-16T13:58:25.707746Z","shell.execute_reply":"2025-02-16T13:58:25.712760Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Download required NLTK data\nnltk.download('stopwords')\nnltk.download('punkt')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T13:58:25.715617Z","iopub.execute_input":"2025-02-16T13:58:25.715872Z","iopub.status.idle":"2025-02-16T13:58:25.743157Z","shell.execute_reply.started":"2025-02-16T13:58:25.715851Z","shell.execute_reply":"2025-02-16T13:58:25.742239Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Config:\n    # File paths\n    input_dir = Path('/kaggle/input/gods2025')\n    output_dir = Path('/kaggle/working/')\n    \n    # Model parameters\n    model_name = 'roberta-base'  # Changed to RoBERTa\n    max_length = 256\n    train_batch_size = 16\n    valid_batch_size = 32\n    epochs = 15\n    learning_rate = 2e-5\n    num_labels = 5\n    \n    # Regularization parameters\n    dropout_prob = 0.1\n    weight_decay = 0.01\n    patience = 3\n    min_delta = 0.001\n    \n    # Class names\n    class_names = [\n        'suicidal-thoughts-and-self-harm',\n        'anxiety',\n        'depression',\n        'relationship-and-family-issues',\n        'ptsd-and-trauma'\n    ]\n    \n    # Path for best model only\n    model_save_path = output_dir / 'best_model.pth'\n    \n    # Class weights (will be calculated)\n    class_weights = None\n\n    checkpoint_dir = output_dir / 'checkpoints'\n    checkpoint_freq = 1  # Save checkpoint every N epochs\n    resume_training = True  # Whether to resume from checkpoint if available\n    \n    def get_checkpoint_path(self, epoch):\n        return self.checkpoint_dir / f'checkpoint_epoch_{epoch}.pt'\n\nconfig = Config()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T13:58:25.744454Z","iopub.execute_input":"2025-02-16T13:58:25.744737Z","iopub.status.idle":"2025-02-16T13:58:25.757839Z","shell.execute_reply.started":"2025-02-16T13:58:25.744696Z","shell.execute_reply":"2025-02-16T13:58:25.757021Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"class CheckpointHandler:\n    def __init__(self, config):\n        self.config = config\n        self.config.checkpoint_dir.mkdir(exist_ok=True)\n        \n    def save_checkpoint(self, epoch, model, optimizer, scheduler, \n                       train_losses, val_losses, best_val_f1):\n        \"\"\"Save a checkpoint of the training state\"\"\"\n        checkpoint = {\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'scheduler_state_dict': scheduler.state_dict(),\n            'train_losses': train_losses,\n            'val_losses': val_losses,\n            'best_val_f1': best_val_f1\n        }\n        torch.save(checkpoint, self.config.get_checkpoint_path(epoch))\n        \n        # Save latest checkpoint reference\n        latest_checkpoint = {\n            'latest_epoch': epoch\n        }\n        torch.save(latest_checkpoint, \n                  self.config.checkpoint_dir / 'latest_checkpoint.pt')\n        \n    def load_latest_checkpoint(self):\n        \"\"\"Load the latest checkpoint if it exists\"\"\"\n        latest_path = self.config.checkpoint_dir / 'latest_checkpoint.pt'\n        if not latest_path.exists():\n            return None\n            \n        latest = torch.load(latest_path)\n        checkpoint_path = self.config.get_checkpoint_path(latest['latest_epoch'])\n        \n        if not checkpoint_path.exists():\n            return None\n            \n        return torch.load(checkpoint_path)\n        \n    def clean_old_checkpoints(self, current_epoch):\n        \"\"\"Remove checkpoints older than the last 2 epochs\"\"\"\n        for path in self.config.checkpoint_dir.glob('checkpoint_epoch_*.pt'):\n            epoch = int(path.stem.split('_')[-1])\n            if epoch < current_epoch - 2:\n                path.unlink()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T13:58:25.758893Z","iopub.execute_input":"2025-02-16T13:58:25.759250Z","iopub.status.idle":"2025-02-16T13:58:25.774666Z","shell.execute_reply.started":"2025-02-16T13:58:25.759185Z","shell.execute_reply":"2025-02-16T13:58:25.773892Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Cell 3: Data Preparation\ndef clean_text(text):\n    \"\"\"\n    Enhanced text cleaning function that removes stop words, special characters,\n    and unnecessary content.\n    \n    Args:\n        text (str): Input text to be cleaned\n        \n    Returns:\n        str: Cleaned text\n    \"\"\"\n    # Convert to string and lowercase\n    text = str(text).lower()\n    \n    # Remove URLs\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n    \n    # Remove email addresses\n    text = re.sub(r'\\S+@\\S+', '', text)\n    \n    # Remove phone numbers\n    text = re.sub(r'\\+?[\\d\\s-]{10,}', '', text)\n    \n    # Remove special characters and numbers\n    text = re.sub(r'[^\\w\\s]', ' ', text)\n    text = re.sub(r'\\d+', '', text)\n    text = re.sub(r'\\[Post removed at request of member\\]\\n?', '', text)\n    text = re.sub(r'\\n', ' ', text)      # Remove newlines\n    \n    # Remove multiple spaces\n    text = re.sub(r'\\s+', ' ', text).strip()\n    \n    # Tokenize the text\n    tokens = word_tokenize(text)\n    \n    # Get stop words\n    stop_words = set(stopwords.words('english'))\n    \n    # Additional domain-specific words to remove (customize as needed)\n    additional_stop_words = {\n        'please', 'help', 'think', 'know', 'like', 'would', 'could', \n        'may', 'also', 'many', 'much', 'trying', 'try', 'sure', \n        'way', 'even', 'really', 'lot', 'back', 'since', 'around',\n        'still', 'time', 'always', 'never', 'want', 'wanted', 'needs',\n        'need', 'feel', 'feeling', 'felt'\n    }\n    \n    stop_words.update(additional_stop_words)\n    \n    # Remove stop words and keep only words with length > 2\n    tokens = [word for word in tokens if word not in stop_words and len(word) > 2]\n    \n    # Join tokens back into text\n    cleaned_text = ' '.join(tokens)\n    \n    # Limit text length (optional, adjust as needed)\n    return cleaned_text[:2000]\n\ndef prepare_data():\n    \"\"\"\n    Enhanced data preparation function with improved text cleaning\n    \"\"\"\n    # Load data\n    train_df = pd.read_csv(config.input_dir/'train.csv')\n    test_df = pd.read_csv(config.input_dir/'test.csv')\n    \n    # Handle missing content and apply enhanced cleaning\n    for df in [train_df, test_df]:\n        # Fill missing content with title or [MISSING]\n        df['content'] = df['content'].fillna(df['title'].fillna('[MISSING]'))\n        \n        # Clean content and title separately\n        df['content_cleaned'] = df['content'].apply(clean_text)\n        df['title_cleaned'] = df['title'].fillna('').apply(clean_text)\n        \n        # Combine cleaned content and title\n        df['text'] = df['content_cleaned'] + \" \" + df['title_cleaned']\n        \n        # Remove any double spaces that might have been created\n        df['text'] = df['text'].str.strip().replace(r'\\s+', ' ', regex=True)\n        \n        # Drop temporary columns\n        df.drop(['content_cleaned', 'title_cleaned'], axis=1, inplace=True)\n    \n    # Calculate class weights (rest of the function remains the same)\n    labels = train_df['target'].map(config.class_names.index)\n    config.class_weights = torch.tensor(\n        compute_class_weight('balanced', classes=np.unique(labels), y=labels),\n        dtype=torch.float32\n    )\n    \n    # Split data\n    train_texts, val_texts, train_labels, val_labels = train_test_split(\n        train_df['text'],\n        labels.tolist(),\n        test_size=0.2,\n        stratify=train_df['target'],\n        random_state=42\n    )\n    \n    return (train_texts.tolist(), train_labels,\n            val_texts.tolist(), val_labels,\n            test_df['text'].tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T13:58:25.775440Z","iopub.execute_input":"2025-02-16T13:58:25.775664Z","iopub.status.idle":"2025-02-16T13:58:25.791635Z","shell.execute_reply.started":"2025-02-16T13:58:25.775645Z","shell.execute_reply":"2025-02-16T13:58:25.790749Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"class MentalHealthDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        # RoBERTa uses the same encoding process\n        encoding = self.tokenizer.encode_plus(\n            str(self.texts[idx]),\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n        }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T13:58:25.792445Z","iopub.execute_input":"2025-02-16T13:58:25.792739Z","iopub.status.idle":"2025-02-16T13:58:25.811331Z","shell.execute_reply.started":"2025-02-16T13:58:25.792711Z","shell.execute_reply":"2025-02-16T13:58:25.810621Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"class EarlyStopping:\n    def __init__(self, patience=3, min_delta=0.001):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.counter = 0\n        self.best_loss = None\n        self.early_stop = False\n        \n    def __call__(self, val_loss):\n        if self.best_loss is None:\n            self.best_loss = val_loss\n        elif val_loss > self.best_loss - self.min_delta:\n            self.counter += 1\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_loss = val_loss\n            self.counter = 0\n\n\n\ndef train():\n    print(\"Checking files...\")\n    assert (config.input_dir/'train.csv').exists(), \"train.csv missing!\"\n    assert (config.input_dir/'test.csv').exists(), \"test.csv missing!\"\n    \n    # Initialize checkpoint handler\n    checkpoint_handler = CheckpointHandler(config)\n    \n    # Load and prepare data\n    print(\"Loading and preparing data...\")\n    train_df = pd.read_csv(config.input_dir/'train.csv')\n    \n    # Clean text data\n    train_df['text'] = train_df.apply(\n        lambda x: clean_text(str(x['content']) + \" \" + str(x['title'])),\n        axis=1\n    )\n    \n    # Split data with stratification\n    train_texts, val_texts, train_labels, val_labels = train_test_split(\n        train_df['text'],\n        train_df['target'].map(config.class_names.index),\n        test_size=0.15,\n        stratify=train_df['target'],\n        random_state=42\n    )\n    \n    # Initialize tokenizer and datasets\n    tokenizer = RobertaTokenizer.from_pretrained(config.model_name)\n    \n    train_dataset = MentalHealthDataset(\n        train_texts.tolist(),\n        train_labels.tolist(),\n        tokenizer,\n        config.max_length\n    )\n    val_dataset = MentalHealthDataset(\n        val_texts.tolist(),\n        val_labels.tolist(),\n        tokenizer,\n        config.max_length\n    )\n    \n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=config.train_batch_size,\n        shuffle=True\n    )\n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=config.valid_batch_size\n    )\n    \n    # Initialize model\n    model = RobertaForSequenceClassification.from_pretrained(\n        config.model_name,\n        num_labels=config.num_labels,\n        hidden_dropout_prob=config.dropout_prob,\n        attention_probs_dropout_prob=config.dropout_prob\n    )\n    model = model.to('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    # Calculate class weights\n    class_weights = torch.tensor(\n        compute_class_weight('balanced', \n                           classes=np.unique(train_labels),\n                           y=train_labels),\n        dtype=torch.float32\n    )\n    \n    # Initialize optimizer and scheduler\n    optimizer = AdamW(\n        model.parameters(),\n        lr=config.learning_rate,\n        weight_decay=config.weight_decay\n    )\n    \n    total_steps = len(train_loader) * config.epochs\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=int(0.1 * total_steps),\n        num_training_steps=total_steps\n    )\n    \n    # Initialize training state variables\n    start_epoch = 0\n    train_losses = []\n    val_losses = []\n    best_val_f1 = 0\n    \n    # Load checkpoint if available and resume_training is True\n    if config.resume_training:\n        checkpoint = checkpoint_handler.load_latest_checkpoint()\n        if checkpoint is not None:\n            print(f\"Resuming from epoch {checkpoint['epoch']}\")\n            model.load_state_dict(checkpoint['model_state_dict'])\n            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n            start_epoch = checkpoint['epoch']\n            train_losses = checkpoint['train_losses']\n            val_losses = checkpoint['val_losses']\n            best_val_f1 = checkpoint['best_val_f1']\n    \n    criterion = torch.nn.CrossEntropyLoss(\n        weight=class_weights.to(model.device)\n    )\n    \n    early_stopping = EarlyStopping(\n        patience=config.patience,\n        min_delta=config.min_delta\n    )\n    \n    # Training loop\n    for epoch in range(start_epoch, config.epochs):\n        print(f\"\\nEpoch {epoch+1}/{config.epochs}\")\n        print(\"-\" * 30)\n        \n        # Training phase\n        model.train()\n        total_loss = 0\n        \n        for batch in tqdm(train_loader, desc=\"Training\"):\n            inputs = {k: v.to(model.device) for k, v in batch.items()}\n            outputs = model(**inputs)\n            loss = criterion(outputs.logits, inputs['labels'])\n            \n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            scheduler.step()\n            optimizer.zero_grad()\n            \n            total_loss += loss.item()\n        \n        avg_train_loss = total_loss / len(train_loader)\n        train_losses.append(avg_train_loss)\n        \n        # Validation phase\n        model.eval()\n        val_preds = []\n        val_labels_list = []\n        val_loss = 0\n        \n        with torch.no_grad():\n            for batch in tqdm(val_loader, desc=\"Validating\"):\n                inputs = {k: v.to(model.device) for k, v in batch.items()}\n                outputs = model(**inputs)\n                loss = criterion(outputs.logits, inputs['labels'])\n                val_loss += loss.item()\n                \n                val_preds.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n                val_labels_list.extend(inputs['labels'].cpu().numpy())\n        \n        avg_val_loss = val_loss / len(val_loader)\n        val_losses.append(avg_val_loss)\n        \n        # Calculate metrics\n        val_acc = accuracy_score(val_labels_list, val_preds)\n        val_f1 = f1_score(val_labels_list, val_preds, average='macro')\n        \n        print(f\"Train Loss: {avg_train_loss:.4f}\")\n        print(f\"Val Loss: {avg_val_loss:.4f}\")\n        print(f\"Val Accuracy: {val_acc:.4f}\")\n        print(f\"Val F1-score: {val_f1:.4f}\")\n        \n        # Save checkpoint if needed\n        if (epoch + 1) % config.checkpoint_freq == 0:\n            checkpoint_handler.save_checkpoint(\n                epoch + 1,\n                model,\n                optimizer,\n                scheduler,\n                train_losses,\n                val_losses,\n                best_val_f1\n            )\n            print(f\"Checkpoint saved for epoch {epoch + 1}\")\n            \n            # Clean old checkpoints\n            checkpoint_handler.clean_old_checkpoints(epoch + 1)\n        \n        # Save best model\n        if val_f1 > best_val_f1:\n            best_val_f1 = val_f1\n            torch.save(model.state_dict(), config.model_save_path)\n            print(\"New best model saved!\")\n        \n        # Early stopping check\n        early_stopping(avg_val_loss)\n        if early_stopping.early_stop:\n            print(\"Early stopping triggered!\")\n            break\n    \n    # Plot learning curves\n    plt.figure(figsize=(10, 6))\n    plt.plot(train_losses, label='Training Loss')\n    plt.plot(val_losses, label='Validation Loss')\n    plt.title('Learning Curves')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.savefig(config.output_dir / 'learning_curves.png')\n    plt.close()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T13:58:25.813554Z","iopub.execute_input":"2025-02-16T13:58:25.813801Z","iopub.status.idle":"2025-02-16T13:58:25.833190Z","shell.execute_reply.started":"2025-02-16T13:58:25.813778Z","shell.execute_reply":"2025-02-16T13:58:25.832354Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Updated prediction function\ndef predict():\n    # Load best model\n    model = RobertaForSequenceClassification.from_pretrained(\n        config.model_name,\n        num_labels=config.num_labels\n    )\n    model.load_state_dict(torch.load(config.model_save_path))\n    model = model.to('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    # Prepare test data\n    tokenizer = RobertaTokenizer.from_pretrained(config.model_name)\n    _, _, _, _, test_texts = prepare_data()\n    \n    # Create dataset\n    test_dataset = MentalHealthDataset(test_texts, [0]*len(test_texts), tokenizer, config.max_length)\n    test_loader = DataLoader(test_dataset, batch_size=config.valid_batch_size)\n    \n    # Predict\n    model.eval()\n    predictions = []\n    for batch in tqdm(test_loader, desc=\"Predicting\"):\n        inputs = {k: v.to(model.device) for k, v in batch.items() if k != 'labels'}\n        with torch.no_grad():\n            outputs = model(**inputs)\n        predictions.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n    \n    # Save results\n    test_df = pd.read_csv(config.input_dir/'test.csv')\n    submission = pd.DataFrame({\n        'id': test_df['id'],\n        'target': [config.class_names[p] for p in predictions]\n    })\n    submission.to_csv(config.output_dir/'submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T13:58:25.834197Z","iopub.execute_input":"2025-02-16T13:58:25.834504Z","iopub.status.idle":"2025-02-16T13:58:25.850177Z","shell.execute_reply.started":"2025-02-16T13:58:25.834470Z","shell.execute_reply":"2025-02-16T13:58:25.849563Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# Execution\nif __name__ == '__main__':\n    config.output_dir.mkdir(exist_ok=True)\n    train()\n    predict()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T13:58:25.850891Z","iopub.execute_input":"2025-02-16T13:58:25.851120Z"}},"outputs":[{"name":"stdout","text":"Checking files...\nLoading and preparing data...\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}